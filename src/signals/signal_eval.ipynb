{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import polars as pl\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np \n",
    "import polars_ols\n",
    "from skmisc.loess import loess\n",
    "from matplotlib.gridspec import GridSpec\n",
    "from mpl_bsic import apply_bsic_style, apply_bsic_logo, export_figure\n",
    "import matplotlib.colors as mcolors"
   ],
   "id": "a51e4f9f0742a9e0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Loading Data",
   "id": "1f6cc4f25c0b30b3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "bond_data = pl.read_parquet('../../data/final_data/bond_data_oas.pq')\n",
    "print(bond_data.head())"
   ],
   "id": "c32efd72d5fc05de",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "bond_data = bond_data.with_columns(\n",
    "    spread_duration = pl.col('oas') / pl.col('bond_yield') * pl.col('duration')\n",
    ")"
   ],
   "id": "56bf10679319b547",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Quickly filtering out bonds close to default",
   "id": "8585aeabc440ed95"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "abnormal_prices = bond_data.filter(pl.col('bond_ret') >1).select('cusip').unique()\n",
    "tot_cusips = bond_data.n_unique('cusip')\n",
    "\n",
    "print(\n",
    "    f'we have {len(abnormal_prices)} CUSIPs to remove, out of {tot_cusips} ({len(abnormal_prices) / tot_cusips * 100:.2f}%)'\n",
    ")\n",
    "# Segmenting in Buckets, based on Rating, Industry, Duration"
   ],
   "id": "7241f1bb1c064696",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "abnormal_prices = bond_data.filter(pl.col('bondprc') < 75).select('cusip').unique()\n",
    "tot_cusips = bond_data.n_unique('cusip')\n",
    "\n",
    "print(f'we have {len(abnormal_prices)} CUSIPs to remove, out of {tot_cusips} ({len(abnormal_prices) / tot_cusips * 100:.2f}%)')\n",
    "bond_data = bond_data.filter(~pl.col('cusip').is_in(abnormal_prices))"
   ],
   "id": "bfceb9a505f1afe7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Segmenting in Buckets, based on Rating, Industry, Duration",
   "id": "37d5801879233546"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Rating",
   "id": "f679ab7c153efbb6"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Group by rating macro-categories",
   "id": "dd4d4a13bb305769"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "bond_data_with_ratings = bond_data.with_columns(\n",
    "    rating_bucket=pl\n",
    "        .when(pl.col('rating') <= 7).then(pl.lit('A'))\n",
    "        .when(pl.col('rating') <= 16).then(pl.lit('B'))\n",
    "        .otherwise(pl.lit('B'))\n",
    ")\n",
    "bond_data = bond_data_with_ratings"
   ],
   "id": "1a55de1ccf5d2cca",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Industry",
   "id": "608bb11c3406c26"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "We choose the following macro-groups, based on the first digit of the SIC Code: \n",
    "\n",
    "- 0, 1: Agriculture, Forestry, Fishing (0) + Mining (1) + Construction (1)\n",
    "- 2, 3, 5: Manufacturing (2,3), Whole Sale Trade + Retail Trade (5)\n",
    "- 4, 7, 8, 9: Transportation and Public Utilities (4), Public Administration (9), Service (7, 8)\n",
    "- 6: Finance, Insurance, Real Estate (6)\n"
   ],
   "id": "78634c76fa52f4f6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "bond_data.select('PERMNO', 'sic_code').head(4)",
   "id": "74b154a6e085b4dc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "bond_data = bond_data.with_columns(\n",
    "    industry_bucket=pl.col('sic_code').cast(str).str.slice(0,1).cast(int)\n",
    ").with_columns(\n",
    "    # assign bucket from 1 to 5 for the corresponding macro-group\n",
    "    industry_bucket=pl\n",
    "        .when(pl.col('industry_bucket').is_in([0,1])).then(pl.lit(1))\n",
    "        .when(pl.col('industry_bucket').is_in([2,3,5])).then(pl.lit(2))\n",
    "        .when(pl.col('industry_bucket').is_in([4,9])).then(pl.lit(3))\n",
    "        .when(pl.col('industry_bucket') == 6).then(pl.lit(4))\n",
    "        .when(pl.col('industry_bucket').is_in([7,8])).then(pl.lit(5))\n",
    ")\n",
    "\n",
    "bond_data.select('industry_bucket').head(3)"
   ],
   "id": "9fe9aa91abf157aa",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "bond_data.select('industry_bucket').unique()",
   "id": "347a3357129f71ce",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Duration ",
   "id": "8924f302f00a0341"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "bond_data.select('duration').quantile(0.5)",
   "id": "e90877a5d885699",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "bond_data = bond_data.with_columns(\n",
    "    duration_bucket=pl\n",
    "        .when(pl.col('duration') <= 5.5).then(pl.lit('shortdur'))\n",
    "        .otherwise(pl.lit('longdur'))\n",
    ")\n"
   ],
   "id": "8175fd961446dd81",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Aggregate the buckets in one column",
   "id": "c9599eece89db038"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "bond_data = bond_data.with_columns(\n",
    "    bucket=(pl.col('industry_bucket').cast(str) + '_' + pl.col('rating_bucket') + '_' + pl.col('duration_bucket').cast(str))\n",
    ").drop(['industry_bucket', 'rating_bucket', 'duration_bucket'])"
   ],
   "id": "6a44305d1885d179",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "bond_data.select('bucket').head(3)",
   "id": "635e9328efe7566e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "bonds_per_date_bucket = bond_data.group_by(['date', 'bucket']).n_unique().select('date', 'bucket', 'cusip')\n",
    "\n",
    "bonds_per_date_bucket.head()"
   ],
   "id": "5d91ac0f37402444",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "bonds_per_date_bucket.filter(pl.col('cusip') < 10).shape[0] / bonds_per_date_bucket.shape[0]",
   "id": "467b6c3d1601e147",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Compute Residual Returns",
   "id": "f942eed5b9495093"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def weighted_median_pl(data: pl.type_aliases.Sequence[pl.Series]):\n",
    "    \"\"\"Compute weighted median of observations with Polars\n",
    "\n",
    "    To be used inside map_groups, to which you specify as expressions first the values, then the weights to use.\n",
    "    The function takes two pl.Series: the first is the values to use, the second is the weights. \n",
    "    The function returns the weighted median for the given group of observations\n",
    "    \"\"\"\n",
    "    values, weights = data\n",
    "    values = values.to_numpy()\n",
    "    weights = weights.to_numpy()\n",
    "    i = np.argsort(values) \n",
    "    c = np.cumsum(weights[i])\n",
    "\n",
    "    weighted_median = values[i[np.searchsorted(c, 0.5 * c[-1])]]\n",
    "    return weighted_median\n"
   ],
   "id": "ddf81a14c03fd86a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def compute_residual_returns(df: pl.DataFrame, oas_column: str = 'oas', use_median: bool = False):\n",
    "    \"\"\"Compute Residual return as unadjusted return - systematic return.\n",
    "    \n",
    "    * dxs = Duration * Spread\n",
    "    * Systematic Return = Relative DTS * Weighted average return of bond's bucket\n",
    "    * Relative dxs = Bond DTS / weighted average DTS of bond bucket\n",
    "    \n",
    "    * Residual return = bond_ret_t+1 - Systematic Return\n",
    "    \n",
    "    \"\"\"\n",
    "    df = df.with_columns(\n",
    "        dxs = pl.col('spread_duration') * pl.col(oas_column)\n",
    "    )\n",
    "    \n",
    "    grouped = df.select('bucket', 'dxs', 'exretnc_dur_t+1', 'bond_amount_out').drop_nulls().group_by('bucket')\n",
    "    \n",
    "    if use_median: \n",
    "        weighted_dxs = grouped.agg(\n",
    "            weighted_dxs_median=pl.map_groups(exprs=['dxs', 'bond_amount_out'], function=weighted_median_pl),\n",
    "            weighted_ret_median=pl.map_groups(exprs=['exretnc_dur_t+1', 'bond_amount_out'], function=weighted_median_pl)\n",
    "        )\n",
    "    else: \n",
    "        weighted_dxs = grouped.agg(\n",
    "            weighted_dxs_mean=(pl.col('dxs').dot(pl.col('bond_amount_out')) / pl.sum('bond_amount_out')),\n",
    "            weighted_ret_mean=(pl.col('exretnc_dur_t+1').dot(pl.col('bond_amount_out')) / pl.sum('bond_amount_out')),\n",
    "        )\n",
    "        \n",
    "    dxs_col = 'weighted_dxs_' + ('median' if use_median else 'mean')\n",
    "    ret_col = 'weighted_ret_' + ('median' if use_median else 'mean')\n",
    "    \n",
    "    df = (df.select(pl.all().exclude(dxs_col, ret_col))\n",
    "        .join(weighted_dxs, how='left', on='bucket')\n",
    "        .with_columns(\n",
    "            relative_dxs = pl.col('dxs') / pl.col(dxs_col)\n",
    "        ).with_columns(\n",
    "            systematic_return = pl.col('relative_dxs') * pl.col(ret_col)\n",
    "        ).with_columns(\n",
    "            (pl.col('bond_ret_t+1') - pl.col('systematic_return')).alias('residual_return_t+1'),\n",
    "        ))\n",
    "    \n",
    "    return df"
   ],
   "id": "2d0964c52265e0f8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "bond_data = compute_residual_returns(bond_data, use_median=True, oas_column='oas')",
   "id": "6af88c7eb15a7b75",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "bond_data.filter(pl.col('bond_ret_t+1') > 0.5)",
   "id": "ec96766ab14d5ddd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "bond_data.select('residual_return_t+1').describe()",
   "id": "5d718902fb83552a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Scoring Signals based on the quantile ",
   "id": "87ee54e6308ce71a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Signal Class",
   "id": "7edaeb8ab5992a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class Signal(): \n",
    "    def __init__(self, df: pl.DataFrame, column: str, n_quantiles: int, signal_value_name: str):\n",
    "        self.df = df\n",
    "        self.column = column\n",
    "        self.n_quantiles = n_quantiles\n",
    "        \n",
    "        self.quantile_col_name = f'{column}_quantile'\n",
    "        self.delta_col_name = f'{column}_delta'\n",
    "        self.mean_col_name = f'{column}_bucket_mean'\n",
    "        self.signal_value_name = signal_value_name\n",
    "        self.return_col = 'residual_return_t+1'\n",
    "        \n",
    "        self.transition_probabilities = None\n",
    "        self.spline = None\n",
    "        self.alpha_estimates = None\n",
    "        \n",
    "    def compute_quantiles(self):\n",
    "        \"\"\"Divide the cols in quantiles based on the bucket groups\"\"\"\n",
    "        \n",
    "        quantile_col_name, delta_col_name, mean_col_name = self.quantile_col_name, self.delta_col_name, self.mean_col_name\n",
    "        col = self.column\n",
    "        n_quantiles = self.n_quantiles\n",
    "        df = self.df\n",
    "        \n",
    "        # create the bins and labels to be used in qcut\n",
    "        bins = np.linspace(0,1,n_quantiles+1)[1:]\n",
    "        labels = [str(i+1) for i in range(n_quantiles+1)]\n",
    "        \n",
    "        # compute the mean of the signal for each bucket at each date\n",
    "        col_mean_by_bucket = df.select('date', 'bucket', col).group_by(['date', 'bucket']).agg(\n",
    "            pl.col(col).drop_nans().mean().alias(mean_col_name) # .mean breaks with NaNs, so drop them \n",
    "        )\n",
    "        \n",
    "        # join the dataframes and compute the difference with respect to the group average (computed before)\n",
    "        deltas = (df\n",
    "                  .select(pl.all().exclude(quantile_col_name, mean_col_name))\n",
    "                  .join(col_mean_by_bucket, how='left', on=['date', 'bucket'])\n",
    "                  .with_columns(\n",
    "                        (pl.col(col) - pl.col(mean_col_name)).alias(delta_col_name)\n",
    "                    )           \n",
    "                  )\n",
    "        # just select the cols we need, to make computations faster\n",
    "        deltas = deltas.select('date', 'cusip', 'bucket', delta_col_name)\n",
    "    \n",
    "        data = []\n",
    "        total_items = col_mean_by_bucket.shape[0]\n",
    "        print('computing... ', end='')\n",
    "    \n",
    "        i = 0\n",
    "        for group in col_mean_by_bucket.iter_rows(named=True):\n",
    "            i += 1\n",
    "            \n",
    "            # filter for NaNs and Nulls as well \n",
    "            mask = (pl.col('date') == group['date']) & (pl.col('bucket') == group['bucket']) & (pl.col(delta_col_name).is_not_nan() & (pl.col(delta_col_name).is_not_null()))\n",
    "            \n",
    "            group_data = deltas.filter(mask)\n",
    "            # if the DataFrame is emtpy just continue, otherwise qcut will throw an error\n",
    "            if group_data.shape[0] == 0:\n",
    "                continue\n",
    "            \n",
    "            group_data = group_data.with_columns(\n",
    "                pl.col(delta_col_name).qcut(bins,labels=labels, allow_duplicates=True).alias(quantile_col_name)\n",
    "            )\n",
    "                \n",
    "            data.append(group_data)\n",
    "            \n",
    "            if i % 1000 == 0: \n",
    "                print(f'{i}/{total_items} ', end='')\n",
    "        \n",
    "        print('... done!')\n",
    "        \n",
    "        quantiles = pl.concat(data, how='vertical')\n",
    "        quantiles = quantiles.with_columns(\n",
    "            pl.col(quantile_col_name).cast(int).alias(quantile_col_name)\n",
    "        )\n",
    "        \n",
    "        merged_df = df.join(quantiles.drop('bucket'), how='left', on=['date', 'cusip'])\n",
    "        \n",
    "        self.df = merged_df\n",
    "        return merged_df\n",
    "    \n",
    "    def information_ratio(self): \n",
    "        quantile_col, return_col = self.quantile_col_name, self.return_col\n",
    "        if not quantile_col in self.df.columns: \n",
    "            self.compute_quantiles()\n",
    "        \n",
    "        df = self.df \n",
    "        \n",
    "        # group returns by date and the quantile, and compute the average return of the portfolio at the end of every month\n",
    "        signal_monthly_returns = df.group_by(['date', quantile_col]).agg(\n",
    "            pl.col(return_col).drop_nans().mean().alias('mean_return'), # mean return of the portfolio at EoM\n",
    "        ).filter(pl.col(quantile_col).is_not_null())\n",
    "        \n",
    "        signal_monthly_returns = signal_monthly_returns.sort(['date', quantile_col])\n",
    "        \n",
    "        long_short_returns = signal_monthly_returns.pivot(index='date', columns=quantile_col, values='mean_return').with_columns(\n",
    "            (pl.col('10')-pl.col('1')).alias('long_short')\n",
    "        ).select('date', 'long_short')\n",
    "        market_returns = df.group_by('date').agg(pl.col(return_col).mean().alias('mkt_return'))\n",
    "        \n",
    "        portfolio_returns = long_short_returns.join(market_returns, how='left', on='date').with_columns(\n",
    "            delta=(pl.col('long_short')-pl.col('mkt_return'))\n",
    "        )\n",
    "        \n",
    "        information_ratio = portfolio_returns['delta'].mean() / portfolio_returns['delta'].std()\n",
    "        \n",
    "        return information_ratio * np.sqrt(12)\n",
    "\n",
    "    def compute_expected_alpha(self): \n",
    "        quantile_col, return_col = self.quantile_col_name, self.return_col\n",
    "        if not quantile_col in self.df.columns: \n",
    "            self.compute_quantiles()\n",
    "        \n",
    "        df = self.df \n",
    "        \n",
    "        \n",
    "        # group returns by date and the quantile, and compute the average return of the portfolio at the end of every month\n",
    "        monthly_returns_by_bucket = df.group_by(['date', quantile_col]).agg(\n",
    "            pl.col(return_col).drop_nans().mean().alias('mean_return'), # mean return of the portfolio at EoM\n",
    "        )\n",
    "        \n",
    "        final_portfolio_returns = monthly_returns_by_bucket.group_by(quantile_col).agg(\n",
    "            pl.col('mean_return').mean().alias('avg_monthly_return'),\n",
    "            pl.col('mean_return').std().alias('monthly_stdev'),\n",
    "            ((pl.col('mean_return').mean() / pl.col('mean_return').std()) * np.sqrt(12)).alias('sharpe_ratio'),\n",
    "        )\n",
    "        \n",
    "        # insert the key to sort by        \n",
    "        alpha_estimates = final_portfolio_returns.sort(by=quantile_col).drop_nulls().select(quantile_col, 'avg_monthly_return', 'monthly_stdev')\n",
    "        \n",
    "        data = alpha_estimates.select(quantile_col, 'avg_monthly_return').to_numpy()\n",
    "        x, y = data[:, 0], data[:, 1]\n",
    "        \n",
    "        spline = loess(x, y, surface='direct')\n",
    "        spline.fit()\n",
    "        \n",
    "        self.spline = spline    \n",
    "        self.alpha_estimates = alpha_estimates\n",
    "\n",
    "        return alpha_estimates, spline\n",
    "\n",
    "    def fit_alpha_estimates(self):\n",
    "        df, spline = self.df, self.spline\n",
    "        quantile_col = self.quantile_col_name\n",
    "        signal_value_name = self.signal_value_name\n",
    "        \n",
    "        if quantile_col not in df.columns:\n",
    "            df = self.compute_quantiles()\n",
    "            \n",
    "        if spline is None: \n",
    "            self.compute_expected_alpha()\n",
    "            spline = self.spline\n",
    "        \n",
    "        \n",
    "        def f(x: pl.Series): \n",
    "            data = x.to_numpy()\n",
    "            n = len(data)\n",
    "            \n",
    "            values = np.zeros(n)\n",
    "            values[:] = np.nan\n",
    "            \n",
    "            for i in range(n):\n",
    "                val = data[i]\n",
    "                if np.isnan(val):\n",
    "                    continue\n",
    "                signal_val = spline.predict([val]).values\n",
    "                values[i] = signal_val[0]\n",
    "                \n",
    "            return values\n",
    "        \n",
    "        df = df.with_columns(\n",
    "            pl.col(quantile_col).map_batches(function=f, return_dtype=pl.Float64).fill_nan(None).alias(signal_value_name),\n",
    "        ).with_columns(\n",
    "            ((pl.col(signal_value_name) - pl.col(signal_value_name).drop_nulls().mean()) / pl.col(signal_value_name).drop_nulls().std()).alias(signal_value_name)\n",
    "        )\n",
    "        \n",
    "        self.df = df \n",
    "        return df \n",
    "    \n",
    "    \n",
    "    def compute_transition_matrix(self, lag: int = 1):\n",
    "        \"\"\"Compute the transition probabilities for quantiles for a given signal, given a certain lag (1 unit of lag is 1 month of time)\"\"\"\n",
    "        def _update_transition_prob_matrix(chain, counts_matrix, lag):\n",
    "            flat_coords = np.ravel_multi_index((chain[:-lag], chain[lag:]), counts_matrix.shape)\n",
    "            return np.bincount(flat_coords, minlength=counts_matrix.size).reshape(counts_matrix.shape)\n",
    "    \n",
    "        def _convert_to_probabilities(transition_matrix: np.ndarray): \n",
    "            M = transition_matrix.copy()\n",
    "            for row in M: \n",
    "                n = sum(row)\n",
    "                if n > 0: \n",
    "                    row[:] = [f/n for f in row] \n",
    "            \n",
    "            return M\n",
    "        \n",
    "        quantile_col, n_quantiles = self.quantile_col_name, self.n_quantiles\n",
    "        if not quantile_col in self.df.columns: \n",
    "            self.compute_quantiles()\n",
    "            \n",
    "        df = self.df\n",
    "        # group by the cusip, and get the evolution of the quantile during time for each bond\n",
    "        quantiles_evolution = df.sort('date').filter(pl.col(quantile_col).is_not_null()).group_by('cusip', maintain_order=True).agg(pl.col(quantile_col))\n",
    "        \n",
    "        # initialize the matrix of transition probabilities\n",
    "        transition_probabilities = np.zeros((n_quantiles, n_quantiles), dtype=np.float64)\n",
    "        \n",
    "        # iterate through the different evolutions and update the transition probabilities\n",
    "        for row in quantiles_evolution.iter_rows(): \n",
    "            chain = row[1]\n",
    "            if np.min(chain) != 0: \n",
    "                chain -= np.min(chain)\n",
    "            \n",
    "            transition_probabilities[:, :] += _update_transition_prob_matrix(chain, transition_probabilities, lag)\n",
    "        \n",
    "        # convert these to actual probabilities\n",
    "        transition_probabilities = _convert_to_probabilities(transition_probabilities)\n",
    "        self.transition_probabilities = transition_probabilities\n",
    "        \n",
    "        return transition_probabilities\n",
    "        \n",
    "    def plot(self, title: str, probabilities_lag: int = 1): \n",
    "        quantile_col = self.quantile_col_name \n",
    "        n_quantiles = self.n_quantiles\n",
    "        \n",
    "        if self.transition_probabilities is None: \n",
    "            self.compute_transition_matrix(probabilities_lag)\n",
    "        if self.spline is None: \n",
    "            self.compute_expected_alpha()\n",
    "        \n",
    "        transition_probs = self.transition_probabilities\n",
    "        alpha_estimates, spline = self.alpha_estimates, self.spline\n",
    "        \n",
    "        fig = plt.figure(figsize=(10,8))\n",
    "        gs = GridSpec(2,2, figure=fig, bottom=0.1, top=0.9, width_ratios=(3, 2), hspace=0.3) \n",
    "        \n",
    "        barchart_ax = fig.add_subplot(gs[0,0])\n",
    "        probs_ax = fig.add_subplot(gs[0,1])\n",
    "        spline_ax = fig.add_subplot(gs[1,:])\n",
    "        apply_bsic_style(fig, sources=['WRDS', 'openbondassetpricing'])\n",
    "        \n",
    "        bsic_cmap = mcolors.LinearSegmentedColormap.from_list(\n",
    "            \"bsic\", [\"#8EC6FF\", \"#38329A\", \"#0E0B54\", \"#601E66\"]\n",
    "        )\n",
    "        \n",
    "        # plot the barchart\n",
    "        barchart_ax.bar(x=alpha_estimates[quantile_col], height=alpha_estimates['avg_monthly_return'] * 1e2, alpha=0.75)\n",
    "        volatilities_ax = barchart_ax.twinx()\n",
    "        volatilities_ax.set_ylabel('Monthly Volatility (bps)')\n",
    "        volatilities_ax.plot(alpha_estimates[quantile_col], alpha_estimates['monthly_stdev'] * 1e4, label='Volatility', color=bsic_constants.BSIC_COLORS[2], linestyle=(0,(5,10)), marker='o')\n",
    "        volatilities_ax.legend()\n",
    "        \n",
    "        barchart_ax.set_title('Average Monthly Return by Quantile', **bsic_constants.TITLE_STYLE)\n",
    "        barchart_ax.set_xlabel('Quantile')\n",
    "        barchart_ax.set_ylabel('Expected Monthly Alpha (%)')\n",
    "        \n",
    "        # plot the transition probability matrix\n",
    "        probs_ax.imshow(transition_probs, cmap=bsic_cmap, interpolation='nearest')\n",
    "        probs_ax.set_axis_off()\n",
    "        probs_ax.set_title(f'Transition Probabilities ({probabilities_lag}Mo, %)', **bsic_constants.TITLE_STYLE)\n",
    "        for (j,i), label in np.ndenumerate(transition_probs):\n",
    "            probs_ax.text(i,j,int(label * 100),ha='center',va='center', color='white', fontweight='bold', fontsize=10)\n",
    "        \n",
    "        # plot the spline in the bottom\n",
    "        spline_ax.axhline(y=0, c='black', lw=1)\n",
    "        \n",
    "        x_new = np.linspace(0,n_quantiles, 100)\n",
    "        pred = spline.predict(x_new, stderror=True)\n",
    "        conf = pred.confidence()\n",
    "        \n",
    "        lowess_vals = pred.values\n",
    "        ll = conf.lower\n",
    "        ul = conf.upper\n",
    "        \n",
    "        \n",
    "        spline_ax.fill_between(x_new,ll,ul,alpha=0.3)\n",
    "        spline_ax.scatter(x=alpha_estimates[quantile_col], y=alpha_estimates['avg_monthly_return'], label='Expected Alpha', color=bsic_constants.BSIC_COLORS[2])\n",
    "        spline_ax.plot(x_new, lowess_vals)\n",
    "        \n",
    "        spline_ax.set_title('Fitted LOWESS Estimator, with 95% CI', **bsic_constants.TITLE_STYLE)\n",
    "        spline_ax.set_xlabel('Quantile')\n",
    "        spline_ax.set_ylabel('Expected Monthly Alpha')\n",
    "        \n",
    "        suptitle_style = bsic_constants.TITLE_STYLE.copy() \n",
    "        suptitle_style['fontsize'] = 16\n",
    "        fig.suptitle(title, **suptitle_style)\n",
    "        \n",
    "        # apply_bsic_style(fig, spline_ax)\n",
    "        # apply_bsic_style(fig, spline_ax)\n",
    "        \n",
    "        \n",
    "        # subtitle = \"Expected Alpha and Volatility of Carry Factor, Transition Probabilities and fitted LOWESS curve.\"\n",
    "        # add_title_subtitle(fig, title, subtitle)\n",
    "        return fig\n",
    "        "
   ],
   "id": "4f36adf6dba33681",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Carry",
   "id": "c14e26b5eaf89eaf"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "For carry, we rank each bond based on its OAS. ",
   "id": "2bdd49af2f200f8b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "carry_signal = Signal(bond_data, 'oas', 10, 'carry_signal_value')\n",
    "carry_signal.compute_quantiles()\n",
    "ir_carry = carry_signal.information_ratio()\n"
   ],
   "id": "eb1182355717efa1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "fig = carry_signal.plot(title='Carry Factor', probabilities_lag=6)",
   "id": "ea4e471cbf5574e7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "export_figure(fig, '../../exports/carry_factor.svg')",
   "id": "1675599316094e4f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "ir_carry",
   "id": "262f3b7897016d6b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "carry_signal.fit_alpha_estimates()",
   "id": "3464e029cbf91fe5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "bond_data = carry_signal.df\n",
    "\n",
    "bond_data.head()"
   ],
   "id": "a8e5c360c942b8a1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "bond_data.select('carry_signal_value')",
   "id": "89210cd81edf5453",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Momentum (Excess Credit Return)",
   "id": "33f58e909dee7692"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "We use the **Credit Excess Return**, as defined by the AQR paper, to define momentum.\n",
    "\n",
    "$$\n",
    "CER_t=\\frac{1}{12}Spread_{t-1}-\\text{Spread Duration}_{t-1}\\times(Spread_t-Spread_{t-1})\n",
    "$$\n",
    "\n",
    "Which translated to our data is\n",
    "\n",
    "$$\n",
    "CER_t=\\frac{1}{12}OAS_{t-1}-\\text{duration}_{t-1}\\times(OAS_t-OAS_{t-1})\n",
    "$$\n"
   ],
   "id": "93d876544a98ef69"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "bond_data.select('YIELD', 'bond_yield').null_count()",
   "id": "bf3b3abbce6a72d5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# compute excess credit return at each timestamp\n",
    "bond_data = bond_data.sort(['cusip', 'date'])\n",
    "bond_data = bond_data.with_columns(\n",
    "    excess_credit_ret=(pl.col('oas').shift(1) / 12 - pl.col('spread_duration') * (pl.col('oas') - pl.col('oas').shift(1)))\n",
    ")\n",
    "\n",
    "mask = pl.col('cusip') != pl.col('cusip').shift(1)\n",
    "\n",
    "bond_data = bond_data.with_columns(\n",
    "    excess_credit_ret=pl.when(mask).then(None).otherwise(pl.col('excess_credit_ret'))\n",
    ")"
   ],
   "id": "59b7437845939dfd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# compute the x_month rolling excess return\n",
    "def momentum(df: pl.DataFrame, period: str, min_periods: int, n_months: int):\n",
    "    momentum_returns = df.sort(['cusip', 'date']).rolling('date', group_by='cusip', period=period).agg(\n",
    "        pl.when(pl.col('excess_credit_ret').len() > min_periods).then(pl.col('excess_credit_ret').add(1).product().sub(1)).otherwise(None).alias(f'mom_{n_months}mo'),\n",
    "        pl.when(pl.col('excess_credit_ret').len() > min_periods).then(pl.col('bondprc').last() / pl.col('bondprc').first() - 1).otherwise(None).alias(f'price_mom_{n_months}mo'),\n",
    "    )\n",
    "    return momentum_returns\n",
    "\n",
    "\n",
    "mom_6mo = momentum(bond_data, '5mo20d', 5, 6)\n",
    "mom_12mo = momentum(bond_data, '11mo20d', 9, 12)\n",
    "\n",
    "bond_data = bond_data.join(\n",
    "    mom_6mo,\n",
    "    how='left',\n",
    "    on=['cusip', 'date'],\n",
    ").join(\n",
    "    mom_12mo,\n",
    "    how='left',\n",
    "    on=['cusip', 'date'],\n",
    ")"
   ],
   "id": "9c5498e7e8c8ee09",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "credit_momentum_signal = Signal(bond_data, 'price_mom_12mo', 10, 'credit_momentum_signal_value')\n",
    "\n",
    "credit_momentum_signal.compute_quantiles()"
   ],
   "id": "9db53446fdfce189",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "fig = credit_momentum_signal.plot('Credit Momentum', 6)",
   "id": "1b1f2a3fcccaf155",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "export_figure(fig, '../../exports/momentum_factor.svg')",
   "id": "97c11631d9bd3a71",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "credit_momentum_signal.fit_alpha_estimates()",
   "id": "296984ce4ed31a41",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "bond_data = credit_momentum_signal.df\n",
    "\n",
    "bond_data.select('credit_momentum_signal_value')"
   ],
   "id": "23df5888302dc2e4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Value",
   "id": "95ef4c3988e1d508"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We use **Excess Spread to Peers** as a Value Signal in our strategy.",
   "id": "ce35a477dbdde22"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "oas_bucket_means = bond_data.group_by(['date', 'bucket']).agg(pl.col('oas').drop_nans().mean().alias('ESP_bucket_mean_oas'))",
   "id": "a44bfa927bbf6c75",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "oas_bucket_means",
   "id": "5d7b7e8a5cc1bae2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "bond_data = bond_data.join(oas_bucket_means, on=['date', 'bucket'], how='left')",
   "id": "d87816e90ca1e2a0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "bond_data = bond_data.with_columns( \n",
    "    ESP = pl.col('oas') - pl.col('ESP_bucket_mean_oas'))"
   ],
   "id": "7c86c7bcabcbef92",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "bond_data.select('de_ratio', 'intcov_ratio', 'debt_ebitda').null_count()",
   "id": "7d77b3e16f1a7a2f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "bond_data.head(3)",
   "id": "4798d8ff3b2ba479",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "col = 'ESP'\n",
    "bond_data.filter(pl.col(col).is_nan()).shape"
   ],
   "id": "afa049f706849f03",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "bond_data.select('ESP').null_count()",
   "id": "d742972b909557f8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "bond_data = bond_data.with_columns(\n",
    "    ESP=pl.col('ESP').fill_nan(None)\n",
    ")"
   ],
   "id": "e87d75b06d13d355",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "bond_data.select('ESP', 'debt_ebitda', 'de_ratio', 'intcov_ratio').dtypes",
   "id": "b99ebb21f00c3423",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "bond_data.select('ESP').null_count()",
   "id": "b5d3d7b809e03f98",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "bond_data_with_residuals = bond_data.with_columns(\n",
    "    esp_residual = pl\n",
    "        .col('ESP')\n",
    "        .least_squares\n",
    "        .ols(pl.col(\"debt_ebitda\"), pl.col(\"intcov_ratio\"), pl.col(\"de_ratio\"), add_intercept=True, mode=\"residuals\", null_policy='drop', solve_method='svd')\n",
    "        .over(['date', 'bucket'])\n",
    ")"
   ],
   "id": "57c2024c230940e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "bond_data = bond_data_with_residuals",
   "id": "6873220638c9c7d3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "value_signal = Signal(bond_data, 'esp_residual', 10, 'esp_signal_value')",
   "id": "4c389d84a2486ce5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "fig = value_signal.plot('ESP Factor (Value)', 6)",
   "id": "f3ed4846495810f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "export_figure(fig, '../../exports/value_factor.svg')",
   "id": "7c28bead2e8a4255",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "value_signal.fit_alpha_estimates()",
   "id": "a6946c13a9ec36cc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "bond_data = value_signal.df\n",
    "\n",
    "bond_data.select('esp_signal_value')"
   ],
   "id": "99510588d81b732b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Equity Momentum in Credit",
   "id": "e5763c810aa3b974"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def equity_momentum(df: pl.DataFrame, period: str, min_periods: int, n_months: int):\n",
    "    eq_momentum_returns = df.sort(['cusip', 'date']).rolling('date', group_by='cusip', period=period).agg(\n",
    "        pl.when(pl.col('equity_month_return').len() > min_periods).then(pl.col('equity_month_return').add(1).product().sub(1)).otherwise(np.nan).alias(f'eq_mom_{n_months}mo'),\n",
    "        )\n",
    "    return eq_momentum_returns\n",
    "\n",
    "eq_mom_6mo = equity_momentum(bond_data, '5mo20d', 5, 6)\n",
    "eq_mom_12mo = equity_momentum(bond_data, '11mo20d', 9, 12)\n",
    "\n",
    "bond_data = bond_data.join(\n",
    "    eq_mom_6mo,\n",
    "    how='left',\n",
    "    on=['cusip', 'date'],\n",
    ").join(\n",
    "    eq_mom_12mo,\n",
    "    how='left',\n",
    "    on=['cusip', 'date'],\n",
    ")"
   ],
   "id": "661489d233fa2bd0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "bond_data.head(2)",
   "id": "52c5ed5be83fff79",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "equity_momentum_signal = Signal(bond_data, 'eq_mom_12mo', 10, 'equity_momentum_signal_value')",
   "id": "5a8ac61f37b33508",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "fig = equity_momentum_signal.plot('Equity Momentum in Credit', 6)",
   "id": "d18aff13d5d575ba",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "export_figure(fig, '../../exports/equity_momentum_factor.svg')",
   "id": "fda917bfbeb1261d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "equity_momentum_signal.fit_alpha_estimates()",
   "id": "a4fd71f1bcfa61ef",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "bond_data = equity_momentum_signal.df\n",
    "\n",
    "bond_data.select('equity_momentum_signal_value')"
   ],
   "id": "bf59696bbae873b8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Factors Correlation",
   "id": "59c9e02239c6bbd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import seaborn as sns\n",
    "import pandas as pd "
   ],
   "id": "3847d8ff234b04a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "factors = bond_data.select('date', 'cusip', 'credit_momentum_signal_value', 'esp_signal_value', 'carry_signal_value', 'equity_momentum_signal_value')\n",
    "factors = factors.to_pandas()\n",
    "factors: pd.DataFrame\n",
    "\n",
    "factors = factors.rename(columns={\n",
    "    'credit_momentum_signal_value': 'Credit Momentum',\n",
    "    'esp_signal_value': 'ESP Factor',\n",
    "    'carry_signal_value': 'Carry Factor',\n",
    "    'equity_momentum_signal_value': 'Equity Momentum'\n",
    "})"
   ],
   "id": "297cf427f76fc621",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "correlations = factors.corr(numeric_only=True)\n",
    "correlations"
   ],
   "id": "d22e340f217274e8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "\n",
    "bsic_cmap = mcolors.LinearSegmentedColormap.from_list(\n",
    "    \"bsic\", [\"#8EC6FF\", \"#38329A\", \"#0E0B54\", \"#601E66\"]\n",
    ")\n",
    "plt.rcParams.update({'axes.grid': False})\n",
    "fig, ax = plt.subplots(figsize=(10,5))\n",
    "\n",
    "mask = np.triu(np.ones_like(correlations, dtype=bool))\n",
    "heatmap = sns.heatmap(correlations, mask=mask, annot=True, cmap=bsic_cmap, square=True)\n",
    "ax.set_title('Factors Correlation')\n",
    "\n",
    "apply_bsic_style(fig, ax)\n",
    "# title = 'Factors Correlations'\n",
    "# subtitle = 'test bla bla bla'\n",
    "fig.subplots_adjust()\n",
    "# add_title_subtitle(fig, title, subtitle)\n",
    "ax.tick_params(\n",
    "    axis='x',          # changes apply to the x-axis\n",
    "    which='both',      # both major and minor ticks are affected\n",
    "    bottom=False,      # ticks along the bottom edge are off\n",
    "    top=False)         # ticks along the top edge are off) # labels along the bottom edge are off\n",
    "ax.tick_params(\n",
    "    axis='y',          # changes apply to the x-axis\n",
    "    which='both',      # both major and minor ticks are affected\n",
    "    right=False,      # ticks along the bottom edge are off\n",
    "    left=False)\n",
    "# ticks along the top edge are off) # labels along the bottom edge are off\n",
    "export_figure(fig, '../../exports/factors_correlations')"
   ],
   "id": "38a55266dc135fc9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Conclusing ",
   "id": "c7a3d0584ee6c218"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "bond_data.head()",
   "id": "ffb8318f77955f8f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "bond_data.null_count() / bond_data.shape[0]",
   "id": "12c8b8ab0cf70071",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "bond_data.write_parquet('../../data/final_data/data_with_signals_all.pq', compression='zstd', compression_level=10)",
   "id": "e437e9691293e4b1",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
