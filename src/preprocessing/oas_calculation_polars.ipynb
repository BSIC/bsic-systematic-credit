{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Install Packages and Imports"
  },
  {
   "cell_type": "code",
   "metadata": {
    "source_hash": "7958d4a9",
    "execution_start": 1713636401514,
    "execution_millis": 4171,
    "deepnote_to_be_reexecuted": false,
    "cell_id": "826a5b5fc1b4420fafc7f1f55421ac45",
    "deepnote_cell_type": "code",
    "ExecuteTime": {
     "end_time": "2024-04-25T23:07:44.909864Z",
     "start_time": "2024-04-25T23:07:43.920876Z"
    }
   },
   "source": "!pip install -r ../../requirements.txt",
   "block_group": "04dfea0dfed241cea8f2be5ac1d2d7df",
   "outputs_reference": "s3:deepnote-cell-outputs-production/62613e20-7b8f-4dd3-852c-5c80ee5c62f3",
   "content_dependencies": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: QuantLib==1.33 in /Users/andreafranceschini/PycharmProjects/bsic-systematic-credit/venv/lib/python3.12/site-packages (from -r ../../requirements.txt (line 1)) (1.33)\r\n",
      "Requirement already satisfied: polars==0.20.22 in /Users/andreafranceschini/PycharmProjects/bsic-systematic-credit/venv/lib/python3.12/site-packages (from -r ../../requirements.txt (line 2)) (0.20.22)\r\n",
      "\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip is available: \u001B[0m\u001B[31;49m23.2.1\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m24.0\u001B[0m\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpip install --upgrade pip\u001B[0m\r\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "metadata": {
    "source_hash": "50693380",
    "execution_start": 1713636405696,
    "execution_millis": 183,
    "deepnote_to_be_reexecuted": false,
    "cell_id": "ce2dc4ce959946c6a66ca0f6c3b71931",
    "deepnote_cell_type": "code",
    "ExecuteTime": {
     "end_time": "2024-04-25T23:07:46.724450Z",
     "start_time": "2024-04-25T23:07:45.965852Z"
    }
   },
   "source": [
    "import polars as pl\n",
    "import QuantLib as ql\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "from typing import Literal\n",
    "import math\n",
    "import warnings\n",
    "import datetime"
   ],
   "block_group": "c8d92469db31480d8b485f671d3cc160",
   "outputs_reference": null,
   "content_dependencies": null,
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "formattedRanges": [],
    "cell_id": "c54177e5579d40e6a9d562efd813694a",
    "deepnote_cell_type": "text-cell-h1"
   },
   "source": "# Load Bond Returns data and Zero Rate Curve",
   "block_group": "1505f3285a0744ed85e2a44e9a3af107"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Bond Returns + Data Cleaning"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "5a868aabd0b44dc8abff0321bdffb9ef",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "For the definitions of the features, refer to the pdf in the `docs` folder\n",
    "\n",
    "Variable Summary: \n",
    "\n",
    "* `cs`: credit spread computed as bond_yield in excess of duration-matched portfolio of US Treasuries yields\n",
    "* `tmt`: time to maturity (in months, I guess)\n",
    "* `ind_num_17`: Fama-French 17 Industry Classification (may be useful for value factor)\n",
    "* `size_ig`, `size_jk`: dummy for (respectively) IG/HY bonds in the BAML/ICE index\n",
    "* `bond_type`: US Corporate Convertible (CCOV), US Corporate Debentures (CDEB), US Corporate Medium Term Note (CMTN), US Corporate Medium Term Note Zero (CMTZ), or US Corporate Paper (CP)\n",
    "* `R_FR`, `N_FR` and co.: rating as names and one-hot encoded, probably from different providers (SP may be S&P, FR Fitch Ratings, MR Moody's\n",
    "* `INTEREST_FREQUENCY`: e.g. 13 for variable coupon, -1 for NA, 14 for bi-monthly, 15-16 undocumented\n",
    "* `DATED_DATE`: date from which the bond interest accrues\n",
    "* Additional Info on variables: FISD data dictionary 2012 document\n",
    "\n",
    "Prices Variables\n",
    "\n",
    "* `PRICE_EOM`: considers all trading days and takes the last\n",
    "* `PRICE_LDM`: consider only last trading day of the month otherwise NaN\n",
    "* `PRICE_L5M`: consider only last 5 trading days of the month otherwise NaN\n",
    "* `T_SPREAD`: average trade-weighted bid-ask spread\n",
    "* `T_YLD_PT`: average trade-weighted yield point\n",
    "* `T_VOLUME`: volume traded during the month, par-value\n",
    "* `T_DVOLUME`: volume traded during the month, dollar value\n",
    "* `bondprc` is adjusted for MMN, `BONDPRC` is unadjusted\n",
    "\n",
    "Ratings\n",
    "* ['R_SP', 'R_MR', 'R_FR', 'RATING_NUM', 'RATING_CAT', 'RATING_CLASS'] are useless\n",
    "* 'rating' column is the useful one and contains the S&P rating code.   \n",
    "    * A-rated (1-7): AAA (1), AA+ (2), AA (3), AA- (4), A+ (5), A(6), A- (7)\n",
    "    * B-rated (8-16): BBB+ (8), BBB (9), BBB- (10), BB+ (11), BB (12), BB- (13), B+ (14), B (15), B- (16)\n",
    "    * C-rated (17-20): CCC+ (17), CCC (18), CCC- (19), CC (20), C (21)\n",
    "    * D: 22\n",
    "    * NR (not rated): null\n",
    "\n",
    "in the actual series, 1 is AAA, 21 is D\n",
    "    \n",
    "Other notes\n",
    "\n",
    "* We may want to remove defaulted bonds (check if they were actually already removed)"
   ],
   "block_group": "d647787c3090446690a2090286b3cbdf"
  },
  {
   "cell_type": "code",
   "metadata": {
    "source_hash": "7321b315",
    "execution_start": 1713635584941,
    "execution_millis": 4832,
    "sql_integration_id": "deepnote-dataframe-sql",
    "deepnote_variable_name": "bond_data",
    "deepnote_to_be_reexecuted": false,
    "cell_id": "f49c28841be94cc4813865191a239c55",
    "deepnote_cell_type": "sql",
    "deepnote_sql_source": "SELECT * \nFROM '/work/data/bond_data_final.csv'\nLIMIT 1000",
    "ExecuteTime": {
     "end_time": "2024-04-25T23:07:48.806858Z",
     "start_time": "2024-04-25T23:07:48.623012Z"
    }
   },
   "source": [
    "bond_data = pl.read_csv('../data/bond_data_final.csv', try_parse_dates=True)\n",
    "\n",
    "bond_data.columns"
   ],
   "block_group": "8b24d105bf90471cbe6ae408caad5883",
   "outputs_reference": "s3:deepnote-cell-outputs-production/691be501-ea86-4bd6-8383-977aad71ce61",
   "content_dependencies": null,
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "No such file or directory (os error 2): ../data/bond_data_final.csv",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[3], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m bond_data \u001B[38;5;241m=\u001B[39m \u001B[43mpl\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread_csv\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43m../data/bond_data_final.csv\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtry_parse_dates\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[1;32m      3\u001B[0m bond_data\u001B[38;5;241m.\u001B[39mcolumns\n",
      "File \u001B[0;32m~/PycharmProjects/bsic-systematic-credit/venv/lib/python3.12/site-packages/polars/_utils/deprecation.py:134\u001B[0m, in \u001B[0;36mdeprecate_renamed_parameter.<locals>.decorate.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    129\u001B[0m \u001B[38;5;129m@wraps\u001B[39m(function)\n\u001B[1;32m    130\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mwrapper\u001B[39m(\u001B[38;5;241m*\u001B[39margs: P\u001B[38;5;241m.\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs: P\u001B[38;5;241m.\u001B[39mkwargs) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m T:\n\u001B[1;32m    131\u001B[0m     _rename_keyword_argument(\n\u001B[1;32m    132\u001B[0m         old_name, new_name, kwargs, function\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m, version\n\u001B[1;32m    133\u001B[0m     )\n\u001B[0;32m--> 134\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunction\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/bsic-systematic-credit/venv/lib/python3.12/site-packages/polars/_utils/deprecation.py:134\u001B[0m, in \u001B[0;36mdeprecate_renamed_parameter.<locals>.decorate.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    129\u001B[0m \u001B[38;5;129m@wraps\u001B[39m(function)\n\u001B[1;32m    130\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mwrapper\u001B[39m(\u001B[38;5;241m*\u001B[39margs: P\u001B[38;5;241m.\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs: P\u001B[38;5;241m.\u001B[39mkwargs) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m T:\n\u001B[1;32m    131\u001B[0m     _rename_keyword_argument(\n\u001B[1;32m    132\u001B[0m         old_name, new_name, kwargs, function\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m, version\n\u001B[1;32m    133\u001B[0m     )\n\u001B[0;32m--> 134\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunction\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/bsic-systematic-credit/venv/lib/python3.12/site-packages/polars/_utils/deprecation.py:134\u001B[0m, in \u001B[0;36mdeprecate_renamed_parameter.<locals>.decorate.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    129\u001B[0m \u001B[38;5;129m@wraps\u001B[39m(function)\n\u001B[1;32m    130\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mwrapper\u001B[39m(\u001B[38;5;241m*\u001B[39margs: P\u001B[38;5;241m.\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs: P\u001B[38;5;241m.\u001B[39mkwargs) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m T:\n\u001B[1;32m    131\u001B[0m     _rename_keyword_argument(\n\u001B[1;32m    132\u001B[0m         old_name, new_name, kwargs, function\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m, version\n\u001B[1;32m    133\u001B[0m     )\n\u001B[0;32m--> 134\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunction\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/bsic-systematic-credit/venv/lib/python3.12/site-packages/polars/io/csv/functions.py:416\u001B[0m, in \u001B[0;36mread_csv\u001B[0;34m(source, has_header, columns, new_columns, separator, comment_prefix, quote_char, skip_rows, dtypes, schema, null_values, missing_utf8_is_empty_string, ignore_errors, try_parse_dates, n_threads, infer_schema_length, batch_size, n_rows, encoding, low_memory, rechunk, use_pyarrow, storage_options, skip_rows_after_header, row_index_name, row_index_offset, sample_size, eol_char, raise_if_empty, truncate_ragged_lines, decimal_comma)\u001B[0m\n\u001B[1;32m    404\u001B[0m         dtypes \u001B[38;5;241m=\u001B[39m {\n\u001B[1;32m    405\u001B[0m             new_to_current\u001B[38;5;241m.\u001B[39mget(column_name, column_name): column_dtype\n\u001B[1;32m    406\u001B[0m             \u001B[38;5;28;01mfor\u001B[39;00m column_name, column_dtype \u001B[38;5;129;01min\u001B[39;00m dtypes\u001B[38;5;241m.\u001B[39mitems()\n\u001B[1;32m    407\u001B[0m         }\n\u001B[1;32m    409\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m prepare_file_arg(\n\u001B[1;32m    410\u001B[0m     source,\n\u001B[1;32m    411\u001B[0m     encoding\u001B[38;5;241m=\u001B[39mencoding,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    414\u001B[0m     storage_options\u001B[38;5;241m=\u001B[39mstorage_options,\n\u001B[1;32m    415\u001B[0m ) \u001B[38;5;28;01mas\u001B[39;00m data:\n\u001B[0;32m--> 416\u001B[0m     df \u001B[38;5;241m=\u001B[39m \u001B[43m_read_csv_impl\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    417\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdata\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    418\u001B[0m \u001B[43m        \u001B[49m\u001B[43mhas_header\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mhas_header\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    419\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcolumns\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcolumns\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mcolumns\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01melse\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mprojection\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    420\u001B[0m \u001B[43m        \u001B[49m\u001B[43mseparator\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mseparator\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    421\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcomment_prefix\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcomment_prefix\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    422\u001B[0m \u001B[43m        \u001B[49m\u001B[43mquote_char\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mquote_char\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    423\u001B[0m \u001B[43m        \u001B[49m\u001B[43mskip_rows\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mskip_rows\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    424\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdtypes\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdtypes\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    425\u001B[0m \u001B[43m        \u001B[49m\u001B[43mschema\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mschema\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    426\u001B[0m \u001B[43m        \u001B[49m\u001B[43mnull_values\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mnull_values\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    427\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmissing_utf8_is_empty_string\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmissing_utf8_is_empty_string\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    428\u001B[0m \u001B[43m        \u001B[49m\u001B[43mignore_errors\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mignore_errors\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    429\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtry_parse_dates\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtry_parse_dates\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    430\u001B[0m \u001B[43m        \u001B[49m\u001B[43mn_threads\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mn_threads\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    431\u001B[0m \u001B[43m        \u001B[49m\u001B[43minfer_schema_length\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minfer_schema_length\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    432\u001B[0m \u001B[43m        \u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbatch_size\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    433\u001B[0m \u001B[43m        \u001B[49m\u001B[43mn_rows\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mn_rows\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    434\u001B[0m \u001B[43m        \u001B[49m\u001B[43mencoding\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mencoding\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mencoding\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m==\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mutf8-lossy\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01melse\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mutf8\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m    435\u001B[0m \u001B[43m        \u001B[49m\u001B[43mlow_memory\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlow_memory\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    436\u001B[0m \u001B[43m        \u001B[49m\u001B[43mrechunk\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrechunk\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    437\u001B[0m \u001B[43m        \u001B[49m\u001B[43mskip_rows_after_header\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mskip_rows_after_header\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    438\u001B[0m \u001B[43m        \u001B[49m\u001B[43mrow_index_name\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrow_index_name\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    439\u001B[0m \u001B[43m        \u001B[49m\u001B[43mrow_index_offset\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrow_index_offset\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    440\u001B[0m \u001B[43m        \u001B[49m\u001B[43msample_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msample_size\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    441\u001B[0m \u001B[43m        \u001B[49m\u001B[43meol_char\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43meol_char\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    442\u001B[0m \u001B[43m        \u001B[49m\u001B[43mraise_if_empty\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mraise_if_empty\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    443\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtruncate_ragged_lines\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtruncate_ragged_lines\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    444\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdecimal_comma\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdecimal_comma\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    445\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    447\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m new_columns:\n\u001B[1;32m    448\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m _update_columns(df, new_columns)\n",
      "File \u001B[0;32m~/PycharmProjects/bsic-systematic-credit/venv/lib/python3.12/site-packages/polars/io/csv/functions.py:559\u001B[0m, in \u001B[0;36m_read_csv_impl\u001B[0;34m(source, has_header, columns, separator, comment_prefix, quote_char, skip_rows, dtypes, schema, null_values, missing_utf8_is_empty_string, ignore_errors, try_parse_dates, n_threads, infer_schema_length, batch_size, n_rows, encoding, low_memory, rechunk, skip_rows_after_header, row_index_name, row_index_offset, sample_size, eol_char, raise_if_empty, truncate_ragged_lines, decimal_comma)\u001B[0m\n\u001B[1;32m    555\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(msg)\n\u001B[1;32m    557\u001B[0m projection, columns \u001B[38;5;241m=\u001B[39m parse_columns_arg(columns)\n\u001B[0;32m--> 559\u001B[0m pydf \u001B[38;5;241m=\u001B[39m \u001B[43mPyDataFrame\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread_csv\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    560\u001B[0m \u001B[43m    \u001B[49m\u001B[43msource\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    561\u001B[0m \u001B[43m    \u001B[49m\u001B[43minfer_schema_length\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    562\u001B[0m \u001B[43m    \u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    563\u001B[0m \u001B[43m    \u001B[49m\u001B[43mhas_header\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    564\u001B[0m \u001B[43m    \u001B[49m\u001B[43mignore_errors\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    565\u001B[0m \u001B[43m    \u001B[49m\u001B[43mn_rows\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    566\u001B[0m \u001B[43m    \u001B[49m\u001B[43mskip_rows\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    567\u001B[0m \u001B[43m    \u001B[49m\u001B[43mprojection\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    568\u001B[0m \u001B[43m    \u001B[49m\u001B[43mseparator\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    569\u001B[0m \u001B[43m    \u001B[49m\u001B[43mrechunk\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    570\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcolumns\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    571\u001B[0m \u001B[43m    \u001B[49m\u001B[43mencoding\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    572\u001B[0m \u001B[43m    \u001B[49m\u001B[43mn_threads\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    573\u001B[0m \u001B[43m    \u001B[49m\u001B[43mpath\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    574\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdtype_list\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    575\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdtype_slice\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    576\u001B[0m \u001B[43m    \u001B[49m\u001B[43mlow_memory\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    577\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcomment_prefix\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    578\u001B[0m \u001B[43m    \u001B[49m\u001B[43mquote_char\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    579\u001B[0m \u001B[43m    \u001B[49m\u001B[43mprocessed_null_values\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    580\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmissing_utf8_is_empty_string\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    581\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtry_parse_dates\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    582\u001B[0m \u001B[43m    \u001B[49m\u001B[43mskip_rows_after_header\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    583\u001B[0m \u001B[43m    \u001B[49m\u001B[43mparse_row_index_args\u001B[49m\u001B[43m(\u001B[49m\u001B[43mrow_index_name\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrow_index_offset\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    584\u001B[0m \u001B[43m    \u001B[49m\u001B[43msample_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msample_size\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    585\u001B[0m \u001B[43m    \u001B[49m\u001B[43meol_char\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43meol_char\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    586\u001B[0m \u001B[43m    \u001B[49m\u001B[43mraise_if_empty\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mraise_if_empty\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    587\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtruncate_ragged_lines\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtruncate_ragged_lines\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    588\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdecimal_comma\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdecimal_comma\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    589\u001B[0m \u001B[43m    \u001B[49m\u001B[43mschema\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mschema\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    590\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    591\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m wrap_df(pydf)\n",
      "\u001B[0;31mFileNotFoundError\u001B[0m: No such file or directory (os error 2): ../data/bond_data_final.csv"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# merge duration across the two dataframes\n",
    "\n",
    "bond_data = bond_data.with_columns((pl.col('DURATION_y').fill_null(pl.col('DURATION_x'))).alias('DURATION'))\n",
    "bond_data = bond_data.drop(['DURATION_x', 'DURATION_y'])\n",
    "\n",
    "bond_data.head(1)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Remove the columns we know we don't need"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "useless_cols = [\n",
    "    'company_symbol', # we do not need it\n",
    "    'TREASURY_MATURITY', # we do not need it\n",
    "    'CONV', # convertible bonds (we already removed all convertibles) \n",
    "    'sic_code', # SIC Industry Code (we don't need it) \n",
    "    'mom6_1', # 6m momentum (we don't need it) \n",
    "    'ltrev48_12', # sum of bond returns over 48months (momentum) \n",
    "    'TMT', # time to maturity in years (we have it in months in tmt) and TMT has NaNs while tmt does not\n",
    "    'DATE', # has NaNs while date does not\n",
    "    'CUSIP', # has Nans while cusip does not\n",
    "    'COUPON', # repeated in coupon and COUPON has NaNs while coupon does not\n",
    "]\n",
    "useless_rating_cols = ['R_SP', 'R_MR', 'R_FR', 'RATING_NUM', 'RATING_CAT', 'RATING_CLASS']\n",
    "\n",
    "bond_data = bond_data.drop(useless_cols + useless_rating_cols)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Some data is missing in some dates observations, but it is the same for all bonds. So, we fill the `null` values using the other date observations"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# fill all DATED_DATE, OFFERING_DATE, MATURITY, and DAY_COUNT_BASIS since they are all the same for each issue\n",
    "cols_to_fill = ['DATED_DATE', 'OFFERING_DATE', 'DAY_COUNT_BASIS', 'MATURITY', 'NCOUPS', 'FIRST_INTEREST_DATE', 'DEFAULTED']\n",
    "grouped = bond_data.group_by('cusip')\n",
    "\n",
    "def fill_dates(group): \n",
    "    return group.with_columns(group.select(cols_to_fill).fill_null(strategy='forward').fill_null(strategy='backward'))\n",
    "    \n",
    "filled_data = grouped.map_groups(fill_dates).drop_nulls(subset=['DATED_DATE', 'OFFERING_DATE', 'MATURITY', 'NCOUPS'])\n",
    "n_dropped_cusips = bond_data.n_unique('cusip') - filled_data.n_unique('cusip')\n",
    "\n",
    "print(f'Removed {n_dropped_cusips} CUSIPs')\n",
    "bond_data = filled_data"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "bond_data.select(cols_to_fill + ['coupon', 'date']).null_count()",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Remove the bonds which defaulted (`DEFAULTED==\"N\"`), since we don't want to consider distressed debt. The column is the same for each cusip and indicates if the bond defaulted at any time from offering date through maturity"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "bond_data.select('DEFAULTED').unique()",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "bond_data = bond_data.filter(pl.col('DEFAULTED') == 'N')",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Remove extra long maturities (>30 years) and bonds maturing in <1y"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "mask_long_maturities = pl.col('tmt') / 12 < 30\n",
    "bond_data.filter(mask_long_maturities).shape[0] / bond_data.shape[0]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "bond_data = bond_data.filter(mask_long_maturities)",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "mask_short_maturities = pl.col('tmt') > 12\n",
    "bond_data.filter(mask_short_maturities).shape[0] / bond_data.shape[0]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "bond_data = bond_data.filter(mask_short_maturities)",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Remove zero coupon bonds"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "mask_zcb = pl.col('NCOUPS') > 0\n",
    "\n",
    "new_bond_data = bond_data.filter(mask_zcb)\n",
    "\n",
    "removed_cusips = bond_data.n_unique('cusip') - new_bond_data.n_unique('cusip')\n",
    "print(f'Removed {removed_cusips} CUSIPs')\n",
    "\n",
    "bond_data = new_bond_data"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Fix the outlier where the DATED_DATE is after the first coupon"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "bond_data.select(pl.col('FIRST_INTEREST_DATE')).null_count()",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "bond_data_fixed = bond_data.with_columns(\n",
    "    DATED_DATE = pl.when((pl.col('FIRST_INTEREST_DATE') < pl.col('DATED_DATE'))).then(pl.col('OFFERING_DATE')).otherwise(pl.col('DATED_DATE'))\n",
    ")\n",
    "\n",
    "bond_data = bond_data_fixed\n",
    "to_fix_mask = pl.col('FIRST_INTEREST_DATE') < pl.col('OFFERING_DATE')\n",
    "\n",
    "while bond_data.filter(to_fix_mask).n_unique('cusip') > 0:      \n",
    "    bond_data = bond_data.with_columns(\n",
    "        FIRST_INTEREST_DATE=pl\n",
    "            .when(to_fix_mask & (pl.col('NCOUPS') == 2)).then(pl.col('FIRST_INTEREST_DATE').dt.offset_by('6mo'))\n",
    "            .when(to_fix_mask & (pl.col('NCOUPS') == 4)).then(pl.col('FIRST_INTEREST_DATE').dt.offset_by('3mo'))\n",
    "            .when(to_fix_mask & (pl.col('NCOUPS') == 12)).then(pl.col('FIRST_INTEREST_DATE').dt.offset_by('1mo'))\n",
    "            .otherwise(pl.col('FIRST_INTEREST_DATE'))\n",
    "    )"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "bond_data.select(pl.col('FIRST_INTEREST_DATE')).null_count()",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "bond_data.filter(pl.col('FIRST_INTEREST_DATE') < pl.col('OFFERING_DATE')).n_unique('cusip')",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Some more overview of the data"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "bond_data.head()",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "bond_data.select('bondprc').null_count() / bond_data.shape[0]",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Removing Distressed debt"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "We remove the distressed debt outliers for OAS when we compute the signal. We just mark distressed with a new column, which we will not invest in, but we keep the data for later use in the backtest\n",
    "\n",
    "\n",
    "We remove all bonds whose price goes below $70\n",
    "\n",
    "Now two options: \n",
    "* We remove all observations after the price was hit\n",
    "* If it recovers we can start re-investing in it"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "mask = (pl.col('oas') > 0.2) & (pl.col('oas').is_not_nan())\n",
    "new_bond_data.filter(mask).n_unique('cusip')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "mask_price = (pl.col('bondprc') < 70) & (pl.col('bondprc').is_not_nan())\n",
    "new_bond_data.filter(mask).n_unique('cusip')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "new_bond_data.n_unique('cusip')",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "new_bond_data.group_by('cusip').n_unique().filter(pl.col('bondprc') > 36)",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "new_bond_data.filter((pl.col('oas') > 1) & (pl.col('oas').is_not_nan())).group_by('DEFAULTED').n_unique()",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "formattedRanges": [],
    "cell_id": "1e6b94406af04aa5bb9f172b15df2fc6",
    "deepnote_cell_type": "text-cell-h1"
   },
   "source": "# Load the historical zero curve",
   "block_group": "f17c1c52563443a9bc36820fce72448b"
  },
  {
   "cell_type": "code",
   "metadata": {
    "source_hash": "8f44720e",
    "execution_start": 1713636079206,
    "execution_millis": 1574,
    "deepnote_to_be_reexecuted": false,
    "cell_id": "a4c3fca0247f402185a036a18eb5dd45",
    "deepnote_cell_type": "code"
   },
   "source": [
    "yield_curve = pl.read_csv('../data/yield_panel_monthly_frequency_daily_maturity.csv', try_parse_dates=True)\n",
    "yield_curve = yield_curve.drop('MAX_DATA_TTM')"
   ],
   "block_group": "1ab17150e7fa4c7c96c90a3ba349f33f",
   "outputs_reference": null,
   "content_dependencies": null,
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "yield_curve = yield_curve.with_columns(pl.col('').alias('date')).drop('')\n",
    "yield_curve = yield_curve.with_columns(pl.all().exclude('date').cast(float))\n",
    "yield_curve[:3, -3:]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "yield_curve = yield_curve.sort('date').upsample(\n",
    "    time_column='date',\n",
    "    every='1d', \n",
    ").select(pl.all().forward_fill())"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "yield_curve = yield_curve.group_by(pl.col('date').dt.month_end()).agg(pl.all().last()).sort('date')",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "yield_curve[:10, :10]",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "formattedRanges": [],
    "cell_id": "ffc03bdcc0944eb7b22f8df758d70d48",
    "deepnote_cell_type": "text-cell-h1"
   },
   "source": "# OAS Calculation with QuantLib",
   "block_group": "f9d46ba3e10e4e7bb9b560ab8b2c0be0"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "03cb3458ae914d10b6cbddaf90d8d623",
    "deepnote_cell_type": "markdown"
   },
   "source": "OAS is the spread that added to the zero rates in the pricing function returns the price of the bond. We use the Newton method to get a solution for the OAS. In our case, since we stripped bonds with optionality, the OAS is the Z-Spread",
   "block_group": "21e488e742ae4cedbb90e17aeb74885b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def decompose_date(date: datetime.date):\n",
    "    \"\"\"\n",
    "    Returns day, month, year given a `pd.Timestamp`\n",
    "    Parameters\n",
    "    ----------\n",
    "    date: pd.Timestamp\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    tuple[int, int, int]: day, month, year\n",
    "\n",
    "    \"\"\"\n",
    "    return date.day, date.month, date.year\n",
    "\n",
    "def get_day_count(bond: dict): \n",
    "    day_count_convention = bond['DAY_COUNT_BASIS']\n",
    "    \n",
    "    if day_count_convention == '30/360': \n",
    "        return ql.Thirty360(ql.Thirty360.USA)\n",
    "    elif day_count_convention == 'ACT/360': \n",
    "        return ql.Actual360()\n",
    "    elif day_count_convention == 'ACT/ACT': \n",
    "        return ql.ActualActual(ql.ActualActual.Bond)\n",
    "    \n",
    "    raise Exception(f'we did not implement day count {day_count_convention}')\n",
    "    \n",
    "def get_coupon_freq(bond: dict): \n",
    "    coupon_freq = bond['NCOUPS']\n",
    "    if coupon_freq == 1: \n",
    "        return ql.Period(ql.Annual)\n",
    "    elif coupon_freq == 2: \n",
    "        return ql.Period(ql.Semiannual)\n",
    "    elif coupon_freq == 4: \n",
    "        return ql.Period(ql.Quarterly)\n",
    "    elif coupon_freq == 12: \n",
    "        return ql.Period(ql.Monthly)\n",
    "    \n",
    "    raise Exception(f'we did not implement coupon freq {coupon_freq}')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def get_zero_curve(date: ql.Date | datetime.date) -> ql.ZeroCurve: \n",
    "    # get the zero rates for that specific date\n",
    "    if isinstance(date, ql.Date):\n",
    "        date_polar = date.to_date()\n",
    "    elif isinstance(date, datetime.date): \n",
    "        date_polar = date\n",
    "    else: \n",
    "        raise Exception('unknown type for date,', type(date))\n",
    "    \n",
    "    zero_rates = yield_curve.row(by_predicate=(pl.col('date') == date_polar))\n",
    "    if not isinstance(zero_rates[0], datetime.date): \n",
    "        raise Exception('we are cutting yields from the calculation')\n",
    "    zero_rates = list(zero_rates[1:])\n",
    "    # create the list of tenors based on the number of observations\n",
    "    tenors = np.arange(0, len(zero_rates) + 1)\n",
    "    \n",
    "    # set the tenor unit and compounding frequency based on the type of data used\n",
    "    tenor_unit = ql.Days\n",
    "    \n",
    "    # create the list of spot dates and rates\n",
    "    #   (need to add a point for the evaluation date, hence the 0.)\n",
    "    spot_dates = [date + ql.Period(tenor.item(), tenor_unit) for tenor in tenors] \n",
    "    spot_rates = [0.] + zero_rates\n",
    "    \n",
    "    # set payment convention as specified in the paper (365 days)\n",
    "    pmt_convention = ql.Actual365Fixed(ql.Actual365Fixed.Standard)\n",
    "    \n",
    "    # create the ZeroCurve and return it\n",
    "    calendar = ql.UnitedStates(ql.UnitedStates.SOFR)\n",
    "    spot_curve = ql.ZeroCurve(spot_dates, spot_rates, pmt_convention, calendar, ql.Linear(), ql.Compounded, ql.Continuous)\n",
    "    \n",
    "    return spot_curve\n",
    "\n",
    "def map_zero_curves(zero_curve: pl.Series): \n",
    "    date = zero_curve[0]\n",
    "    \n",
    "    if not isinstance(zero_curve[0], datetime.date): \n",
    "        raise Exception('we are cutting yields from the calculation')\n",
    "    \n",
    "    zero_rates = list(zero_curve[1:])\n",
    "    # create the list of tenors based on the number of observations\n",
    "    tenors = np.arange(0, len(zero_rates) + 1)\n",
    "    \n",
    "    # set the tenor unit and compounding frequency based on the type of data used\n",
    "    tenor_unit = ql.Days\n",
    "    \n",
    "    # create the list of spot dates and rates\n",
    "    #   (need to add a point for the evaluation date, hence the 0.)\n",
    "    spot_dates = [ql.Date(date.day, date.month, date.year) + ql.Period(tenor.item(), tenor_unit) for tenor in tenors] \n",
    "    spot_rates = [0.] + zero_rates\n",
    "    \n",
    "    # set payment convention as specified in the paper (365 days)\n",
    "    pmt_convention = ql.Actual365Fixed(ql.Actual365Fixed.Standard)\n",
    "    \n",
    "    # create the ZeroCurve and return it\n",
    "    calendar = ql.UnitedStates(ql.UnitedStates.SOFR)\n",
    "    spot_curve = ql.ZeroCurve(spot_dates, spot_rates, pmt_convention, calendar, ql.Linear(), ql.Compounded, ql.Continuous)\n",
    "    \n",
    "    return date, spot_curve\n",
    "    \n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Cache the Zero Curves for each EoM in a dictionary so to not recompute them every time"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# get zero curves for all days\n",
    "zero_curves = dict()\n",
    "for data in yield_curve.filter(pl.col('date') >= datetime.date(2000,1,1)).iter_rows(): \n",
    "    date, curve = map_zero_curves(data)\n",
    "    zero_curves[date] = curve   "
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "len(zero_curves)",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Final Function for OAS Calculation"
  },
  {
   "cell_type": "code",
   "metadata": {
    "source_hash": "e7f8963",
    "execution_start": 1713635912409,
    "execution_millis": 81,
    "deepnote_to_be_reexecuted": false,
    "cell_id": "115bf260678f4c5cb90b2f8657a0989e",
    "deepnote_cell_type": "code"
   },
   "source": [
    "def debug_cashflows(bond: ql.FixedRateBond, bond_data: pl.Series, mkt_price: float, z_spread: float, impl_clean_price: float):\n",
    "    \"\"\"\n",
    "    Debug cashflows given a bond and bond_data. \n",
    "    \n",
    "    Function to debug the results of the OAS calcuations.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    bond\n",
    "    bond_data\n",
    "    \"\"\"\n",
    "    cashflows = bond.cashflows()\n",
    "    print('--- BOND SETUP & CALCS CHECKS ---')\n",
    "    print(f'\\tCalc Date = {bond_data.date}, \\n\\tOffering date = {bond_data.OFFERING_DATE}, Maturity = {bond_data.MATURITY}')\n",
    "    \n",
    "    # check for coupon_amt\n",
    "    data_coupon_amt = bond_data.coupon * 100 / bond_data.NCOUPS / 100 # todo account for the coupon frequency\n",
    "    bond_ql_coup_amt = np.round(cashflows[2].amount(), 2)\n",
    "    print(f'\\tCoupon Check: Data = {data_coupon_amt}, Model = {bond_ql_coup_amt}')\n",
    "        \n",
    "    # check that Accrued Interest\n",
    "    data_accrued_interest = bond_data.COUPACC\n",
    "    bond_ql_accr_interest = np.round(bond.dirtyPrice() - bond.cleanPrice(), 2)\n",
    "    print(f'\\tAccrued Interest Check: Data = {data_accrued_interest}, Model = {bond_ql_accr_interest}')\n",
    "    \n",
    "    print('\\tCASHFLOWS SCHEDULE')\n",
    "    for c in cashflows:\n",
    "        print('\\t%20s %12f' % (c.date(), c.amount()))\n",
    "        \n",
    "    coupons = [ql.as_coupon(c) for c in bond.cashflows()[:-1]]\n",
    "    coupons_df = pd.DataFrame([(c.date().to_date(), c.rate(), c.accrualPeriod()) for c in coupons], columns=['date', 'rate', 'accrual_period'], index=range(1,len(coupons)+1))\n",
    "    print(coupons_df)\n",
    "    \n",
    "    # checks for coupon dates\n",
    "    bond_first_pmt_date = bond_data.FIRST_INTEREST_DATE.date()\n",
    "    bond_last_pmt_date = bond_data.LAST_INTEREST_DATE.date()\n",
    "    bond_ql_first_pmt_date = cashflows[0].date().to_date()\n",
    "    bond_ql_last_pmt_date = cashflows[-3].date().to_date()\n",
    "    \n",
    "    first_delta = (bond_first_pmt_date - bond_ql_first_pmt_date).days\n",
    "    last_delta = (bond_last_pmt_date - bond_ql_last_pmt_date).days\n",
    "    \n",
    "    print('\\tChecks for Payment Dates')\n",
    "    print(f'\\t\\tFirst pmt: Data = {bond_first_pmt_date}, Model = {bond_ql_first_pmt_date}, Delta = {first_delta}')\n",
    "    print(f'\\t\\tLast pmt: Data = {bond_last_pmt_date}, Model = {bond_ql_last_pmt_date}, Delta = {last_delta}')\n",
    "    \n",
    "    \n",
    "    delta_p = mkt_price - impl_clean_price\n",
    "    delta_bps = delta_p / mkt_price * 100 * 100\n",
    "\n",
    "    print(f'\\tZ-SPREAD = {z_spread:.5f} ({z_spread * 100:.3f}%)')\n",
    "    print(f'\\tMkt Price = {mkt_price}, Implied Clean Price = {impl_clean_price:.5f}, Delta = {delta_p:.5f}, Delta (bps): {delta_bps:.2f}')\n",
    "    \n",
    "    print(f'DEBUG: {data_coupon_amt} {bond_ql_coup_amt}')\n",
    "    assert math.isclose(data_coupon_amt, bond_ql_coup_amt, rel_tol=1e-2)\n",
    "    if not math.isclose(data_accrued_interest, bond_ql_accr_interest): \n",
    "        warnings.warn('Accrued Interest is not correct')\n",
    "    # assert math.isclose(data_accrued_interest, bond_ql_accr_interest) \n",
    "    assert bond_first_pmt_date == bond_ql_first_pmt_date # check the first payment date matches\n",
    "    print('--- ALL CHECKS PASSED FOR BOND CALCULATIONS ---')\n",
    "    print('--- CHECKS FOR Z-SPREAD CALCULATIONS ---')\n",
    "    assert abs(first_delta) < 3\n",
    "    assert abs(last_delta) < 3\n",
    "    # delta p less than 1bp\n",
    "    assert abs(delta_p) < 0.05\n",
    "    print('--- ALL CHECKS PASSED FOR Z-SPREAD CALCULATION ---')\n",
    "    \n",
    "class ParameterNaNException(Exception):\n",
    "    def __init__(self, varname: str):\n",
    "        self.msg = f'Variable {varname} is NaN, and it is required.'\n",
    "        super().__init__(self.msg)\n",
    "        \n",
    "def check_parameters(bond: pl.Series): \n",
    "    for varname in ['coupon', 'principal_amt']: \n",
    "        if np.isnan(bond[varname]): raise ParameterNaNException(varname)\n",
    "    \n",
    "def compute_OAS(bond: pl.Series, debug: bool = False):\n",
    "    # check that parameters are defined\n",
    "    print(f'computing OAS for bond {bond['cusip']} at {bond['date']}...', end='')\n",
    "    if bond['bondprc'] is None: \n",
    "        print('No price data, skipping this row')\n",
    "        return np.nan\n",
    "    check_parameters(bond)\n",
    "    \n",
    "    calc_date = ql.Date(*decompose_date(bond['date']))\n",
    "    ql.Settings.instance().evaluationDate = calc_date\n",
    "    \n",
    "    # key data\n",
    "    calendar = ql.UnitedStates(ql.UnitedStates.NYSE) # calendar to follow for calculations\n",
    "    calendar = ql.NullCalendar()\n",
    "    # day_count_convention = get_day_count(bond) # the day count convention as specified in the bond\n",
    "    day_count_convention = ql.ActualActual(ql.ActualActual.Bond) # the day count convention as specified in the bond\n",
    "    \n",
    "    # bond data\n",
    "    issue_date = ql.Date(*decompose_date(bond['OFFERING_DATE']))\n",
    "    accruing_start_date = ql.Date(*decompose_date(bond['DATED_DATE'])) # this is the date from which the bond starts accruing interest\n",
    "    maturity_date = ql.Date(*decompose_date(bond['MATURITY']))\n",
    "    tenor = get_coupon_freq(bond)\n",
    "    date_generation = ql.DateGeneration.Backward\n",
    "    month_end = False\n",
    "    face_value = bond['principal_amt']\n",
    "    face_value = 100\n",
    "    coupon = bond['coupon'] / 100\n",
    "    mkt_price = bond['bondprc']\n",
    "    first_pmt_date = ql.Date(*decompose_date(bond['FIRST_INTEREST_DATE']))\n",
    "    \n",
    "    schedule = ql.Schedule(accruing_start_date, maturity_date, tenor, calendar, ql.Unadjusted, ql.Unadjusted, date_generation, month_end, first_pmt_date)\n",
    "    \n",
    "    settlement_days = 0\n",
    "    \n",
    "    # zero curve\n",
    "    spot_curve = zero_curves[bond['date']]\n",
    "    pricing_curve = ql.YieldTermStructureHandle(spot_curve)\n",
    "    \n",
    "    bond_ql = ql.FixedRateBond(\n",
    "        settlement_days, \n",
    "        face_value, \n",
    "        schedule, \n",
    "        [coupon],\n",
    "        day_count_convention\n",
    "    )\n",
    "    bond_ql.setPricingEngine(ql.DiscountingBondEngine(pricing_curve))\n",
    "    \n",
    "    # Z-spread calculation \n",
    "    z_spread = ql.BondFunctions.zSpread(\n",
    "        bond_ql, \n",
    "        mkt_price,\n",
    "        spot_curve,\n",
    "        day_count_convention, \n",
    "        ql.Compounded,\n",
    "        ql.Continuous, \n",
    "        calc_date,\n",
    "        1.e-16,\n",
    "        10_000_000,\n",
    "        0.\n",
    "    )\n",
    "    \n",
    "    def get_impl_clean_price(spread):\n",
    "        spread1 = ql.SimpleQuote(spread)\n",
    "        spread_handle1 = ql.QuoteHandle(spread1)\n",
    "        ts_spreaded1 = ql.ZeroSpreadedTermStructure(pricing_curve,\n",
    "                                                    spread_handle1,\n",
    "                                                    ql.Compounded,\n",
    "                                                    ql.Continuous)\n",
    "        ts_spreaded_handle1 = ql.YieldTermStructureHandle(ts_spreaded1)\n",
    "        fixed_rate_bond = ql.FixedRateBond(settlement_days,\n",
    "                                        face_value,\n",
    "                                        schedule,\n",
    "                                        [coupon],\n",
    "                                        day_count_convention)\n",
    "        # Set Valuation engine\n",
    "        bond_engine = ql.DiscountingBondEngine(ts_spreaded_handle1)\n",
    "        fixed_rate_bond.setPricingEngine(bond_engine)\n",
    "        value = fixed_rate_bond.cleanPrice()\n",
    "        print(f'bond NPV: {fixed_rate_bond.NPV()}, clean: {fixed_rate_bond.cleanPrice()}')\n",
    "        return value\n",
    "    \n",
    "    if debug: \n",
    "        impl_clean_price = get_impl_clean_price(z_spread)\n",
    "        debug_cashflows(bond_ql, bond, mkt_price, z_spread, impl_clean_price)\n",
    "        \n",
    "    print(f' ...Z-spread is {z_spread}')\n",
    "\n",
    "    return z_spread"
   ],
   "block_group": "694a69b778c449628414f12500f10b77",
   "outputs_reference": null,
   "content_dependencies": null,
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Compute OAS for a Random Bond"
  },
  {
   "cell_type": "code",
   "metadata": {
    "source_hash": "b6c11a54",
    "execution_start": 1713635920962,
    "execution_millis": 106,
    "deepnote_to_be_reexecuted": false,
    "cell_id": "7a74ec40beb94d12abba0f65aa333c17",
    "deepnote_cell_type": "code"
   },
   "source": [
    "example_bond = bond_data.row(10, named=True)\n",
    "spot_crv = compute_OAS(example_bond)"
   ],
   "block_group": "9a921ee166384369a114b5d10438ea89",
   "outputs_reference": null,
   "content_dependencies": null,
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Run the function on the whole DataFrame"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "bond_data = bond_data.sort(['date', 'cusip'])",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "new_bond_data = bond_data.with_columns(oas=pl.Series(compute_OAS(bond) for bond in bond_data.iter_rows(named=True)))",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "bond_data = new_bond_data",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "bond_data.write_parquet('../data/bond_data_with_oas.pq', compression='zstd', compression_level=5)",
   "outputs": [],
   "execution_count": null
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "deepnote_persisted_session": {
   "createdAt": "2024-04-20T17:49:10.133Z"
  },
  "deepnote_full_width": true,
  "deepnote_notebook_id": "76025adab9af4a4fb451a04a4bf44042",
  "deepnote_execution_queue": [],
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 }
}
